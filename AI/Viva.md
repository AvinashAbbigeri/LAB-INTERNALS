Viva Questions based on each lab program
1. Explore pre-trained word vectors
• What are word embeddings?
• What is vector arithmetic in NLP?
• How do pre-trained models like Word2Vec or GloVe learn embeddings?
• What does the result of "king - man + woman" imply?
• What is the significance of cosine similarity in comparing word vectors?
2. Dimensionality reduction and word relationships
• Why do we use dimensionality reduction for word embeddings?
• Differentiate between PCA and t-SNE.
• How can clusters in a t-SNE plot help in understanding word relationships?
• What does it mean for words to be “semantically similar”?
• How do you generate semantically similar words programmatically?
3. Train a custom Word2Vec model
• What is the difference between skip-gram and CBOW?
• What are the key parameters for training a Word2Vec model?
• Why do domain-specific embeddings improve performance in certain tasks?
• How would you evaluate the quality of your trained word embeddings?
• What preprocessing steps are required before training?
4. Improve GenAI prompts using embeddings
• How do word embeddings enrich prompts for Generative AI?
• What is prompt engineering? Why is it important?
• How does adding contextually similar words affect model responses?
• What metrics can you use to compare original vs. enriched outputs?
• Can you explain semantic similarity with an example?
5. Word embeddings for creative writing
• How can word embeddings help in generating creative content?
• What is a seed word? How are similar words generated from it?
• Why is context important when generating a paragraph?
• What’s the difference between syntactic and semantic similarity?
• How can this technique be used in story generation?
6. Sentiment analysis using Hugging Face
• What is sentiment analysis?
• How does a Hugging Face transformer model perform sentiment analysis?
• What is a sentiment analysis pipeline?
• What are positive, negative, and neutral sentiments in NLP?
• How would you handle sarcastic or ambiguous input?
7. Text summarization using Hugging Face
• What is text summarization?
• Differentiate between extractive and abstractive summarization.
• What pre-trained models are used for summarization?
• What are the limitations of automatic summarization?
• How would you evaluate the quality of a summary?
8. LangChain and Cohere integration
• What is LangChain used for?
• How do LLM frameworks interact with APIs like Cohere?
• What is the role of prompt templates in LangChain?
• How does the API key mechanism work for secure access?
• What are the benefits of loading documents from Google Drive?
9. Structured output using Pydantic and Wikipedia data
• What is Pydantic and why is it used in NLP pipelines?
• How does structured data parsing work with LLMs?
• What is an output parser and how is it implemented?
• What fields did you extract from Wikipedia and how?
• Why is schema definition important in output consistency?
10. Chatbot for Indian Penal Code
• What steps are needed to build a chatbot for a legal document?
• How do you ensure accuracy when answering legal questions?
What are the challenges of using LLMs for legal data?
• How do you convert static text (IPC) into a queryable format?
• What tools or models can be used for building legal chatbots?
General Questions on Generative AI
? Basic Concepts
1. What is Generative AI?
2. How does Generative AI differ from traditional AI models?
3. Give examples of Generative AI models.
4. What is the role of deep learning in Generative AI?
5. What is the difference between discriminative and generative models?
? Popular Models
6. What is GPT and how does it work?
7. How does a transformer architecture work in Generative AI?
8. What are LLMs (Large Language Models)? Give examples.
9. Explain the use of diffusion models in generative tasks.
10. What is the role of embeddings in LLMs?
? Applications
11. List real-world applications of Generative AI.
12. How is Generative AI used in content creation?
13. What are some medical or legal applications of Generative AI?
14. How is Generative AI transforming education or customer support?
15. Can Generative AI be used in design and art? How?
? Prompt Engineering
16. What is a prompt in Generative AI?
17. What is prompt tuning or engineering?
18. How do you write an effective prompt for a language model?
19. How do zero-shot, one-shot, and few-shot prompting differ?
20. What is a temperature parameter in text generation?
? Ethics and Limitations
21. What are the ethical concerns related to Generative AI?
22. What is hallucination in Generative AI models?
23. Can Generative AI be biased? How?
24. What are risks of using Generative AI in legal or financial domains?
25. What steps can be taken to ensure responsible use of Generative AI?
? Technical Understanding
26. What is fine-tuning in the context of Generative AI?
27. How are tokenization and embeddings used in text generation?
28. How do LLMs handle multilingual input/output?
29. What is a context window in LLMs and why is it important?
30. What is Reinforcement Learning with Human Feedback (RLHF)?
